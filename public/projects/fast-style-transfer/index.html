<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Fast Style Transfer üèéüí®üñåÔ∏èüé®üß† | Mujtaba</title>
<meta name="keywords" content="">
<meta name="description" content="Background üìñüìï In this project, I will do a PyTorch implemention of the fast neural style transfer algorithm described in the paper Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
This method essentially involves training a model to approximate the optimization based neural style transfer. The benefit is that it runs about 3 orders of magnitude faster!
Because of its improved inference time, it&rsquo;s feasible to run style transfer in real time as you&rsquo;ll also.">
<meta name="author" content="">
<link rel="canonical" href="https://igreat.github.io/projects/fast-style-transfer/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css" integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://igreat.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://igreat.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://igreat.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://igreat.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://igreat.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Fast Style Transfer üèéüí®üñåÔ∏èüé®üß†" />
<meta property="og:description" content="Background üìñüìï In this project, I will do a PyTorch implemention of the fast neural style transfer algorithm described in the paper Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
This method essentially involves training a model to approximate the optimization based neural style transfer. The benefit is that it runs about 3 orders of magnitude faster!
Because of its improved inference time, it&rsquo;s feasible to run style transfer in real time as you&rsquo;ll also." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://igreat.github.io/projects/fast-style-transfer/" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2023-01-01T20:45:01&#43;04:00" />
<meta property="article:modified_time" content="2023-01-01T20:45:01&#43;04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Fast Style Transfer üèéüí®üñåÔ∏èüé®üß†"/>
<meta name="twitter:description" content="Background üìñüìï In this project, I will do a PyTorch implemention of the fast neural style transfer algorithm described in the paper Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
This method essentially involves training a model to approximate the optimization based neural style transfer. The benefit is that it runs about 3 orders of magnitude faster!
Because of its improved inference time, it&rsquo;s feasible to run style transfer in real time as you&rsquo;ll also."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "https://igreat.github.io/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Fast Style Transfer üèéüí®üñåÔ∏èüé®üß†",
      "item": "https://igreat.github.io/projects/fast-style-transfer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Fast Style Transfer üèéüí®üñåÔ∏èüé®üß†",
  "name": "Fast Style Transfer üèéüí®üñåÔ∏èüé®üß†",
  "description": "Background üìñüìï In this project, I will do a PyTorch implemention of the fast neural style transfer algorithm described in the paper Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi, and Li Fei-Fei.\nThis method essentially involves training a model to approximate the optimization based neural style transfer. The benefit is that it runs about 3 orders of magnitude faster!\nBecause of its improved inference time, it\u0026rsquo;s feasible to run style transfer in real time as you\u0026rsquo;ll also.",
  "keywords": [
    
  ],
  "articleBody": "Background üìñüìï In this project, I will do a PyTorch implemention of the fast neural style transfer algorithm described in the paper Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi, and Li Fei-Fei.\nThis method essentially involves training a model to approximate the optimization based neural style transfer. The benefit is that it runs about 3 orders of magnitude faster!\nBecause of its improved inference time, it‚Äôs feasible to run style transfer in real time as you‚Äôll also. The github repository for this project can be found here.\nTo navigate where you want to go, here are the contents:\nBackground üìñüìï Results üòé Control and TradeOffs ‚öñÔ∏è Video Stylization üé•üñåÔ∏è Comparison with Optimization Based Method Setting This Up On Your Computer How To Style Your Image Or Video How To Train Your Own Model Results üòé Control and Tradeoffs ‚öñÔ∏è Training the model involves a bunch of hyperparameters which include:\nStyle weight Content weight TV regularization to improve smoothness I‚Äôve found that when leaving the content weight as 1e2, a style weight ranging from 1e7 or 1e8 works well, but it depends on the style image.\nThough I‚Äôve kept the total variation regulizer, I found that leaving its weight at 0 (disabling it) yields consistently better results. However, perhaps if it‚Äôs sufficiently small it can be good (the lua implementation from the original paper had it).\nVideo Stylization üé•üñåÔ∏è Because of its vastly improved inference time, we can run fast neural style transfer in videos, even in real time!\nComparison with Optimization Based Method Though fast neural style transfer runs about three orders of magnitude faster than its optimization-based counterpart, it produces noticebly less quality styled images.\nSetting This Up On Your Computer Soon to be implemented üí™ (I just need to package all dependencies into a file for people to download)\nHow To Style Your Image Or Video You‚Äôll have to first navigate to the project file in your terminal.\nStylize Your Image To style your own image, here is the most basic command you can write:\npython stylize_image.py --image_path {PATH TO YOUR VIDEO} --pretrained_model {ONE OF MY PRETRAINED MODELS} For example, to generate bahla‚Äôs fort in the style of the starry night, you have to type the following command:\npython stylize_image.py --image_path images/content_images/bahla-fort.jpg --pretrained_model starry_night For more options, here‚Äôs what the help message gives:\nusage: Stylize an image [-h] --image_path IMAGE_PATH [--image_size IMAGE_SIZE] [--pretrained_model {starry_night,rain_princess,abstract,mosaic}] [--model_path MODEL_PATH] [--save_path SAVE_PATH] optional arguments: -h, --help show this help message and exit --image_path IMAGE_PATH path to the image to be stylized --image_size IMAGE_SIZE size of the image to be stylized. if not specified, the image will not be resized --pretrained_model {starry_night,rain_princess,abstract,mosaic} pretrained model to be used for stylizing the image --model_path MODEL_PATH path to the model to be used for stylizing the image --save_path SAVE_PATH path to save the stylized image Note that either a pretrained model or a model path need to be specified.\nStylize Your Video To stylize your own video, here is the most basic command you can write:\npython stylize_video.py --video_path {PATH TO YOUR VIDEO} --pretrained_model {ONE OF MY PRETRAINED MODELS} For example, to generate a video of cat jumping in the style of the starry night, you have to type the following command:\npython stylize_video.py --video_path videos/source_videos/jumping_cat.mp4 --pretrained_model starry_night For more options, here is what the help message gives:\nusage: Stylize a video [-h] --video_path VIDEO_PATH [--pretrained_model {starry_night,rain_princess,abstract,mosaic}] [--model_path MODEL_PATH] [--save_path SAVE_PATH] [--frames_per_step FRAMES_PER_STEP] [--max_image_size MAX_IMAGE_SIZE] optional arguments: -h, --help show this help message and exit --video_path VIDEO_PATH path to the video to be stylized --pretrained_model {starry_night,rain_princess,abstract,mosaic} pretrained model to be used for stylizing the video --model_path MODEL_PATH path to the model to be used for stylizing the video --save_path SAVE_PATH path to save the stylized video --frames_per_step FRAMES_PER_STEP number of frames to transform at a time. higher values will be faster but will result in signficantly more memory usage --max_image_size MAX_IMAGE_SIZE maximum size of dimensions of the video frames. if not specified, the frames will not be resized How To Train Your Own Model I‚Äôve also provided an interface for you to train your own model from scratch. Note that this is very computationally heavy, and unless you have a good GPU and good RAM (12+ GB), be ready for your computer to be taken hostage by the training process.\nYou‚Äôll have to download a large image dataset on your computer to train on. The original paper used the 2014 MS-COCO test dataset (80k images) and trained on it for two epochs. Because all I have is an M1 macbook air, I only trained my models for one epoch, but the results mostly converged.\nHere is the most simple command to train a model:\npython train_model.py --style_image_path {PATH TO YOUR STYLE IMAGE} --train_dataset_path {PATH TO DATASET} For example, to train a model on the 2014 ms-coco datset to transform an image to the style of the starry night, you have to type the following command:\npython train_model.py --style_image_path images/style_images/starry_night.jpg --train_dataset_path data/mscoco You can also monitor the training of your model through tensorboard by typing the following in your terminal:\ntensorboard --logdir=runs --samples_per_plugin images={MAX IMAGES} Note you‚Äôll need multiple terminals for this: one for training your model and one for the tensorboard. I used the terminals provided in VSCODE for the training and my default terminal for tensorboard.\nFor more options, here is what the help message gives:\nusage: Train a model [-h] --style_image_path STYLE_IMAGE_PATH [--train_dataset_path TRAIN_DATASET_PATH] [--save_path SAVE_PATH] [--epochs EPOCHS] [--batch_size BATCH_SIZE] [--image_size IMAGE_SIZE] [--style_size STYLE_SIZE] [--style_weight STYLE_WEIGHT] [--content_weight CONTENT_WEIGHT] [--tv_weight TV_WEIGHT] [--learning_rate LEARNING_RATE] [--checkpoint_path CHECKPOINT_PATH] [--checkpoint_interval CHECKPOINT_INTERVAL] [--device {cpu,cuda,mps}] optional arguments: -h, --help show this help message and exit --style_image_path STYLE_IMAGE_PATH path to the style image --train_dataset_path TRAIN_DATASET_PATH path to the training dataset --save_path SAVE_PATH path to save the trained model --epochs EPOCHS number of epochs to train the model for --batch_size BATCH_SIZE batch size to train the model with --image_size IMAGE_SIZE image size to train the model with --style_size STYLE_SIZE style size to train the model with. if not specified, the orignal size will be used --style_weight STYLE_WEIGHT weight of the style loss --content_weight CONTENT_WEIGHT weight of the content loss --tv_weight TV_WEIGHT weight of the total variation loss --learning_rate LEARNING_RATE learning rate to train the model with --checkpoint_path CHECKPOINT_PATH path to the checkpoint to resume training from. If not specified, training will start from scratch --checkpoint_interval CHECKPOINT_INTERVAL number of images to train on before saving a checkpoint. keep it a multiple of the batch size --device {cpu,cuda,mps} device to train the model on ",
  "wordCount" : "1085",
  "inLanguage": "en",
  "datePublished": "2023-01-01T20:45:01+04:00",
  "dateModified": "2023-01-01T20:45:01+04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://igreat.github.io/projects/fast-style-transfer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Mujtaba",
    "logo": {
      "@type": "ImageObject",
      "url": "https://igreat.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://igreat.github.io/" accesskey="h" title="Mujtaba (Alt + H)">Mujtaba</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://igreat.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://igreat.github.io/videos/" title="Videos">
                    <span>Videos</span>
                </a>
            </li>
            <li>
                <a href="https://igreat.github.io/resume/" title="Resume">
                    <span>Resume</span>
                </a>
            </li>
            <li>
                <a href="https://igreat.github.io/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://igreat.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Fast Style Transfer üèéüí®üñåÔ∏èüé®üß†
    </h1>
    <div class="post-meta"><span title='2023-01-01 20:45:01 +0400 +04'>January 1, 2023</span>

</div>
  </header> 
  <div class="post-content"><h2 id="background-">Background üìñüìï<a hidden class="anchor" aria-hidden="true" href="#background-">#</a></h2>
<p>In this project, I will do a PyTorch implemention of the fast neural style transfer algorithm described in the paper <a href="https://cs.stanford.edu/people/jcjohns/eccv16/">Perceptual Losses for Real-Time Style Transfer
and Super-Resolution</a> by Justin Johnson, Alexandre Alahi, and Li Fei-Fei.</p>
<p>This method essentially involves training a model to <strong>approximate</strong> the <a href="https://github.com/igreat/artistic-style-net">optimization based neural style transfer</a>. The benefit is that it runs about 3 orders of magnitude faster!</p>
<p>Because of its improved inference time, it&rsquo;s feasible to run style transfer in real time as you&rsquo;ll also. The github repository for this project can be found <a href="https://github.com/igreat/fast-style-transfer">here</a>.</p>
<p>To navigate where you want to go, here are the contents:</p>
<ul>
<li><a href="#background-%F0%9F%93%96%F0%9F%93%95">Background üìñüìï</a></li>
<li><a href="#results-%F0%9F%98%8E">Results üòé</a></li>
<li><a href="#control-and-tradeoffs-%E2%9A%96%EF%B8%8F">Control and TradeOffs ‚öñÔ∏è</a></li>
<li><a href="#video-stylization-%F0%9F%8E%A5%F0%9F%96%8C%EF%B8%8F">Video Stylization üé•üñåÔ∏è</a></li>
<li><a href="#comparison-with-optimization-based-method">Comparison with Optimization Based Method</a></li>
<li><a href="#setting-this-up-on-your-computer">Setting This Up On Your Computer</a></li>
<li><a href="#how-to-style-your-image-or-video">How To Style Your Image Or Video</a></li>
<li><a href="#how-to-train-your-own-model">How To Train Your Own Model</a></li>
</ul>
<div align="center">
    <img src="/images/projects/fast-style-transfer/images/style_images/starry-night.jpg" style="display: inline" alt="Starry Night" width="256"/>
    <img src="/images/projects/fast-style-transfer/images/content_images/bahla-fort.jpg" style="display: inline" alt="Bahla Fort" width=256/>
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla-fort-starry-night.png" alt="Starry Grand Mosque" style="display: inline" width="512"/>
</div>
<h2 id="results-">Results üòé<a hidden class="anchor" aria-hidden="true" href="#results-">#</a></h2>
<!-- original images -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/images/content_images/fruits.jpg" style="display: inline" alt="Fruits" width=220/>
    <img src="/images/projects/fast-style-transfer/images/content_images/bahla-fort.jpg" style="display: inline" alt="Bahla Fort" width=220/>
<!-- starry night -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/images/style_images/starry-night.jpg" style="display: inline" width="200" title="starry night">
    <img src="/images/projects/fast-style-transfer/images/generated_images/fruits-starry-night.png" style="display: inline" width="200" title="fruits starry night">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla-fort-starry-night.png" style="display: inline" width="200" title="bahla fort starry night">
</p>
<!-- rain princess -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/images/style_images/rain-princess.jpg" style="display: inline" width="200" title="rain princess">
    <img src="/images/projects/fast-style-transfer/images/generated_images/fruits-rain-princess.png" style="display: inline" width="200" title="fruits rain princess">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla-fort-rain-princess.png" style="display: inline" width="200" title="bahla fort rain princess">
</p>
<!-- abstract dalle -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/images/style_images/mosaic.jpg" style="display: inline" width="200" title="mosaic">
    <img src="/images/projects/fast-style-transfer/images/generated_images/fruits-mosaic.png" style="display: inline" width="200" title="fruits mosaic">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla-fort-mosaic.png" style="display: inline" width="200" title="bahla mosaic">
</p>
<!-- abstract dalle -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/images/style_images/abstract-dalle.png" style="display: inline" width="200" title="abstract dalle">
    <img src="/images/projects/fast-style-transfer/images/generated_images/fruits-abstract.png" style="display: inline" width="200" title="fruits abstract">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla-fort-abstract.png" style="display: inline" width="200" title="bahla fortabstract">
</p>
<h2 id="control-and-tradeoffs-">Control and Tradeoffs ‚öñÔ∏è<a hidden class="anchor" aria-hidden="true" href="#control-and-tradeoffs-">#</a></h2>
<p>Training the model involves a bunch of hyperparameters which include:</p>
<ul>
<li>Style weight</li>
<li>Content weight</li>
<li>TV regularization to improve smoothness</li>
</ul>
<p>I&rsquo;ve found that when leaving the content weight as 1e2, a style weight ranging from 1e7 or 1e8 works well, but it depends on the style image.</p>
<p>Though I&rsquo;ve kept the total variation regulizer, I found that leaving its weight at 0 (disabling it) yields consistently better results. However, perhaps if it&rsquo;s sufficiently small it can be good (the lua implementation from the original paper had it).</p>
<h2 id="video-stylization-">Video Stylization üé•üñåÔ∏è<a hidden class="anchor" aria-hidden="true" href="#video-stylization-">#</a></h2>
<!-- stylized gifs -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/gifs/waving_mosaic.gif" style="display: inline" width="300" title="abstract dalle">
    <img src="/images/projects/fast-style-transfer/gifs/waving_starry_night.gif" style="display: inline" width="300" title="fruits abstract">
    <img src="/images/projects/fast-style-transfer/gifs/waving_rain_princess.gif" style="display: inline" width="300" title="bahla fortabstract">
</p>
<p>Because of its vastly improved inference time, we can run fast neural style transfer in videos, even in real time!</p>
<h2 id="comparison-with-optimization-based-method">Comparison with Optimization Based Method<a hidden class="anchor" aria-hidden="true" href="#comparison-with-optimization-based-method">#</a></h2>
<p>Though fast neural style transfer runs about three orders of magnitude faster than its optimization-based counterpart, it produces noticebly less quality styled images.</p>
<!-- starry night -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/images/style_images/starry-night.jpg" style="display: inline" width="200" title="starry night">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla_starry_night_op.png" style="display: inline" width="200" title="fruits starry night">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla-fort-starry-night.png" style="display: inline" width="200" title="bahla fort starry night">
</p>
<!-- rain princess -->
<p align="center">
    <img src="/images/projects/fast-style-transfer/images/style_images/rain-princess.jpg" style="display: inline" width="200" title="starry night">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla_rain_princess_op.png" style="display: inline" width="200" title="fruits starry night">
    <img src="/images/projects/fast-style-transfer/images/generated_images/bahla-fort-rain-princess.png" style="display: inline" width="200" title="bahla fort starry night">
</p>
<h2 id="setting-this-up-on-your-computer">Setting This Up On Your Computer<a hidden class="anchor" aria-hidden="true" href="#setting-this-up-on-your-computer">#</a></h2>
<p>Soon to be implemented üí™ (I just need to package all dependencies into a file for people to download)</p>
<!-- TODO: package all dependencies into a yaml file -->
<h2 id="how-to-style-your-image-or-video">How To Style Your Image Or Video<a hidden class="anchor" aria-hidden="true" href="#how-to-style-your-image-or-video">#</a></h2>
<p>You&rsquo;ll have to first navigate to the project file in your terminal.</p>
<h3 id="stylize-your-image">Stylize Your Image<a hidden class="anchor" aria-hidden="true" href="#stylize-your-image">#</a></h3>
<p>To style your own image, here is the most basic command you can write:</p>
<pre tabindex="0"><code>python stylize_image.py --image_path {PATH TO YOUR VIDEO} --pretrained_model {ONE OF MY PRETRAINED MODELS}
</code></pre><p>For example, to generate bahla&rsquo;s fort in the style of the starry night, you have to type the following command:</p>
<pre tabindex="0"><code>python stylize_image.py --image_path images/content_images/bahla-fort.jpg --pretrained_model starry_night
</code></pre><p>For more options, here&rsquo;s what the help message gives:</p>
<pre tabindex="0"><code>usage: Stylize an image [-h] --image_path IMAGE_PATH [--image_size IMAGE_SIZE] [--pretrained_model {starry_night,rain_princess,abstract,mosaic}]
                        [--model_path MODEL_PATH] [--save_path SAVE_PATH]

optional arguments:
  -h, --help            show this help message and exit
  --image_path IMAGE_PATH
                        path to the image to be stylized
  --image_size IMAGE_SIZE
                        size of the image to be stylized. if not specified, the image will not be resized
  --pretrained_model {starry_night,rain_princess,abstract,mosaic}
                        pretrained model to be used for stylizing the image
  --model_path MODEL_PATH
                        path to the model to be used for stylizing the image
  --save_path SAVE_PATH
                        path to save the stylized image
</code></pre><p>Note that either a pretrained model or a model path need to be specified.</p>
<h3 id="stylize-your-video">Stylize Your Video<a hidden class="anchor" aria-hidden="true" href="#stylize-your-video">#</a></h3>
<p>To stylize your own video, here is the most basic command you can write:</p>
<pre tabindex="0"><code>python stylize_video.py --video_path {PATH TO YOUR VIDEO} --pretrained_model {ONE OF MY PRETRAINED MODELS}
</code></pre><p>For example, to generate a video of cat jumping in the style of the starry night, you have to type the following command:</p>
<pre tabindex="0"><code>python stylize_video.py --video_path videos/source_videos/jumping_cat.mp4 --pretrained_model starry_night
</code></pre><p>For more options, here is what the help message gives:</p>
<pre tabindex="0"><code>usage: Stylize a video [-h] --video_path VIDEO_PATH [--pretrained_model {starry_night,rain_princess,abstract,mosaic}] [--model_path MODEL_PATH]
                       [--save_path SAVE_PATH] [--frames_per_step FRAMES_PER_STEP] [--max_image_size MAX_IMAGE_SIZE]

optional arguments:
  -h, --help            show this help message and exit
  --video_path VIDEO_PATH
                        path to the video to be stylized
  --pretrained_model {starry_night,rain_princess,abstract,mosaic}
                        pretrained model to be used for stylizing the video
  --model_path MODEL_PATH
                        path to the model to be used for stylizing the video
  --save_path SAVE_PATH
                        path to save the stylized video
  --frames_per_step FRAMES_PER_STEP
                        number of frames to transform at a time. higher values will be faster but will result in signficantly more memory usage
  --max_image_size MAX_IMAGE_SIZE
                        maximum size of dimensions of the video frames. if not specified, the frames will not be resized
</code></pre><h2 id="how-to-train-your-own-model">How To Train Your Own Model<a hidden class="anchor" aria-hidden="true" href="#how-to-train-your-own-model">#</a></h2>
<p>I&rsquo;ve also provided an interface for you to train your own model from scratch. Note that this is very computationally heavy, and unless you have a good GPU and good RAM (12+ GB), be ready for your computer to be taken hostage by the training process.</p>
<p>You&rsquo;ll have to download a large image dataset on your computer to train on. The original paper used the 2014 <a href="https://cocodataset.org/#download">MS-COCO test dataset</a> (80k images) and trained on it for two epochs. Because all I have is an M1 macbook air, I only trained my models for one epoch, but the results <em>mostly</em> converged.</p>
<p>Here is the most simple command to train a model:</p>
<pre tabindex="0"><code>python train_model.py --style_image_path {PATH TO YOUR STYLE IMAGE} --train_dataset_path {PATH TO DATASET}
</code></pre><p>For example, to train a model on the 2014 ms-coco datset to transform an image to the style of the starry night, you have to type the following command:</p>
<pre tabindex="0"><code>python train_model.py --style_image_path images/style_images/starry_night.jpg --train_dataset_path data/mscoco
</code></pre><p>You can also monitor the training of your model through tensorboard by typing the following in your terminal:</p>
<pre tabindex="0"><code>tensorboard --logdir=runs --samples_per_plugin images={MAX IMAGES}
</code></pre><p>Note you&rsquo;ll need multiple terminals for this: one for training your model and one for the tensorboard. I used the terminals provided in VSCODE for the training and my default terminal for tensorboard.</p>
<p>For more options, here is what the help message gives:</p>
<pre tabindex="0"><code>usage: Train a model [-h] --style_image_path STYLE_IMAGE_PATH [--train_dataset_path TRAIN_DATASET_PATH] [--save_path SAVE_PATH] [--epochs EPOCHS]
                     [--batch_size BATCH_SIZE] [--image_size IMAGE_SIZE] [--style_size STYLE_SIZE] [--style_weight STYLE_WEIGHT] [--content_weight CONTENT_WEIGHT]
                     [--tv_weight TV_WEIGHT] [--learning_rate LEARNING_RATE] [--checkpoint_path CHECKPOINT_PATH] [--checkpoint_interval CHECKPOINT_INTERVAL]
                     [--device {cpu,cuda,mps}]

optional arguments:
  -h, --help            show this help message and exit
  --style_image_path STYLE_IMAGE_PATH
                        path to the style image
  --train_dataset_path TRAIN_DATASET_PATH
                        path to the training dataset
  --save_path SAVE_PATH
                        path to save the trained model
  --epochs EPOCHS       number of epochs to train the model for
  --batch_size BATCH_SIZE
                        batch size to train the model with
  --image_size IMAGE_SIZE
                        image size to train the model with
  --style_size STYLE_SIZE
                        style size to train the model with. if not specified, the orignal size will be used
  --style_weight STYLE_WEIGHT
                        weight of the style loss
  --content_weight CONTENT_WEIGHT
                        weight of the content loss
  --tv_weight TV_WEIGHT
                        weight of the total variation loss
  --learning_rate LEARNING_RATE
                        learning rate to train the model with
  --checkpoint_path CHECKPOINT_PATH
                        path to the checkpoint to resume training from. If not specified, training will start from scratch
  --checkpoint_interval CHECKPOINT_INTERVAL
                        number of images to train on before saving a checkpoint. keep it a multiple of the batch size
  --device {cpu,cuda,mps}
                        device to train the model on
</code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://igreat.github.io/">Mujtaba</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
